import os
import re

# Base path for experiments
BASE_PATH = r"C:\hanja_data\Epitext_Project\5_docs\experiments"

# List of experiments with their README info and code files.
# For brevity, only the major experiments are populated with full code; others include placeholders.
experiments = [
    {
        "path": "nlp/sikuroberta/MLM_ì„±ëŠ¥_ë¹„êµ",
        "title": "MLM ì„±ëŠ¥ ë¹„êµ",
        "purpose": "BERT ê¸°ë°˜ í•œë¬¸ MLM ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµ í‰ê°€",
        "summary": "ëª¨ë¸ë³„ Top-K ì •í™•ë„ ë° ì§ˆì  ë¹„êµë¥¼ í†µí•´ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„",
        "files": {
            "main.py": r'''# -*- coding: utf-8 -*-
"""
Historical BERT MLM Evaluation Framework
========================================
Description:
    í•œë¬¸ ë° ê³ ì „ ë¬¸í—Œ ì²˜ë¦¬ì— íŠ¹í™”ëœ 3ê°€ì§€ BERT ëª¨ë¸ì˜ 
    Masked Language Modeling (MLM) ì„±ëŠ¥ì„ ë¹„êµ í‰ê°€í•˜ëŠ” ìë™í™” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

    [ë¹„êµ ëŒ€ìƒ ëª¨ë¸]
    1. SillokBERT (HuggingFace): ì¡°ì„ ì™•ì¡°ì‹¤ë¡ ê¸°ë°˜
    2. SikuRoBERTa (HuggingFace): ì‚¬ê³ ì „ì„œ ê¸°ë°˜
    3. HUE (Local): í•œë¬¸ ê³ ì „ ë¬¸í—Œ ëª¨ë¸ (ì§ì ‘ ë‹¤ìš´ë¡œë“œ í•„ìš”)

Features:
    - ìƒëŒ€ ê²½ë¡œ ì§€ì›: Git Clone í›„ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
    - ìë™ ë°ì´í„°ì…‹ ìƒì„±: í•œì(Hanja) ì‹ë³„ ë° ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹
    - ì´ì¤‘ í‰ê°€: ì •ëŸ‰ì (Top-K Acc) ë° ì •ì„±ì (Side-by-side) ë¹„êµ

Author: [Your Name]
Date: 2025-12-09
Version: 1.2.0 (Relative Path Support)
"""

import os
import re
import time
import random
import logging
from typing import List, Dict, Optional, Any

import torch
from transformers import pipeline, Pipeline

# -----------------------------------------------------------------------------
# 1. Logger Setup (ë¡œê¹… ì„¤ì •)
# -----------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# -----------------------------------------------------------------------------
# 2. Configuration (í™˜ê²½ ì„¤ì •)
# -----------------------------------------------------------------------------
class Config:
    """í”„ë¡œì íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì „ì—­ ì„¤ì • í´ë˜ìŠ¤"""
    # [Project Root] í˜„ì¬ ì‹¤í–‰ íŒŒì¼(main.py)ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    # ëª¨ë¸ ê²½ë¡œ
    SILLOK_MODEL_PATH: str = "ddokbaro/SillokBert"
    SIKU_MODEL_PATH: str = "SIKU-BERT/sikuroberta"
    HUE_MODEL_PATH: str = os.path.join(BASE_DIR, "models", "HUE")
    TEST_DATA_PATH: str = os.path.join(BASE_DIR, "data", "sillok_test.txt")
    TOP_K: int = 5
    NUM_SAMPLES: int = 1000
    DEVICE: int = 0 if torch.cuda.is_available() else -1

# -----------------------------------------------------------------------------
# 3. Model Handler
# -----------------------------------------------------------------------------
class ModelHandler:
    @staticmethod
    def load_pipeline(model_path: str, name: str, top_k: int = 5) -> Optional[Pipeline]:
        """ëª¨ë¸ ê²½ë¡œë¥¼ ë°›ì•„ fill-mask íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        is_local = os.path.isabs(model_path) or os.path.sep in model_path
        if is_local:
            config_path = os.path.join(model_path, "config.json")
            if not os.path.exists(config_path):
                logger.warning(f"âš ï¸  [Skip] '{name}' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                logger.warning(f"    ê²½ë¡œ: {model_path}")
                return None
        try:
            logger.info(f"â³ [{name}] ëª¨ë¸ ë¡œë”© ì¤‘...")
            pipe = pipeline(
                "fill-mask",
                model=model_path,
                tokenizer=model_path,
                device=Config.DEVICE,
                top_k=top_k
            )
            logger.info(f"âœ… [{name}] ë¡œë”© ì™„ë£Œ")
            return pipe
        except Exception as e:
            logger.error(f"âŒ [{name}] ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

# -----------------------------------------------------------------------------
# 4. Data Processor
# -----------------------------------------------------------------------------
class DataProcessor:
    @staticmethod
    def load_and_clean(file_path: str) -> List[str]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ê³  XML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        if not os.path.exists(file_path):
            logger.error(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return []
        clean_lines = []
        patterns = [
            re.compile(r'<[^>]+>'),
            re.compile(r'\([^)]+\)'),
            re.compile(r'\[[^\]]+\]')
        ]
        for line in lines:
            for pat in patterns:
                line = pat.sub('', line)
            line = line.strip()
            if line:
                clean_lines.append(line)
        logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(clean_lines)} ë¬¸ì¥")
        return clean_lines

    @staticmethod
    def create_masked_dataset(lines: List[str], num_samples: int) -> List[Dict[str, str]]:
        """ë¬¸ì¥ ë‚´ í•œì(Hanja)ë¥¼ ì‹ë³„í•˜ì—¬ ë¬´ì‘ìœ„ë¡œ [MASK] í† í°ì„ ì‚½ì…í•©ë‹ˆë‹¤."""
        if not lines:
            return []
        target_lines = lines
        if 0 < num_samples < len(lines):
            target_lines = random.sample(lines, num_samples)
        dataset = []
        for line in target_lines:
            try:
                hanja_indices = [i for i, char in enumerate(line) if '\u4e00' <= char <= '\u9fff']
                if not hanja_indices:
                    continue
                mask_idx = random.choice(hanja_indices)
                answer = line[mask_idx]
                masked_text = line[:mask_idx] + "[MASK]" + line[mask_idx+1:]
                dataset.append({"masked_text": masked_text, "answer": answer})
            except Exception:
                continue
        logger.info(f"ğŸ› ï¸  ë§ˆìŠ¤í‚¹ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: ìœ íš¨ ìƒ˜í”Œ {len(dataset)}ê°œ")
        return dataset

# -----------------------------------------------------------------------------
# 5. Evaluator
# -----------------------------------------------------------------------------
class Evaluator:
    @staticmethod
    def evaluate_quantitative(name: str, pipe: Pipeline, dataset: List[Dict], top_k: int) -> Dict[str, Any]:
        hits_top1 = 0
        hits_topk = 0
        valid_count = 0
        max_len = getattr(pipe.tokenizer, 'model_max_length', 512)
        logger.info(f"ğŸ“Š [{name}] í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ {len(dataset)}ê°œ)...")
        start_time = time.time()
        for item in dataset:
            text = item["masked_text"]
            answer = item["answer"]
            if len(text) > max_len:
                continue
            try:
                results = pipe(text, top_k=top_k)
                if results and isinstance(results[0], list):
                    results = results[0]
                preds = [res['token_str'].strip() for res in results]
                if not preds:
                    continue
                if preds[0] == answer:
                    hits_top1 += 1
                if answer in preds:
                    hits_topk += 1
                valid_count += 1
            except:
                continue
        duration = time.time() - start_time
        if valid_count == 0:
            return {"name": name, "acc_top1": 0.0, "acc_topk": 0.0, "count": 0}
        return {
            "name": name,
            "acc_top1": (hits_top1 / valid_count) * 100,
            "acc_topk": (hits_topk / valid_count) * 100,
            "count": valid_count,
            "duration": duration
        }

    @staticmethod
    def compare_qualitative(sample: Dict, pipes: Dict[str, Pipeline], top_k: int):
        text = sample["masked_text"]
        answer = sample["answer"]
        print("\n" + "="*80)
        print(f"ğŸ§ [Qualitative Analysis] ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ")
        print(f"â€¢ Input Context: {text}")
        print(f"â€¢ Correct Answer: [{answer}]")
        print("-" * 80)
        model_names = list(pipes.keys())
        col_width = 20
        header = f"| Rank |"
        for name in model_names:
            display_name = name[:col_width]
            header += f" {display_name:<{col_width}} |"
        print(header)
        print("|:----:|" + ("-" * (col_width+2) + "|") * len(model_names))
        results_map = {}
        for name, pipe in pipes.items():
            try:
                res = pipe(text, top_k=top_k)
                if res and isinstance(res[0], list):
                    res = res[0]
                results_map[name] = res
            except:
                results_map[name] = []
        for i in range(top_k):
            row_str = f"| {i+1:<4} |"
            for name in model_names:
                res_list = results_map.get(name, [])
                if i < len(res_list):
                    token = res_list[i]['token_str'].strip()
                    score = res_list[i]['score']
                    display = f"{token} ({score:.3f})"
                else:
                    display = "-"
                row_str += f" {display:<{col_width}} |"
            print(row_str)
        print("="*80 + "\n")

# -----------------------------------------------------------------------------
# 6. Main Execution
# -----------------------------------------------------------------------------
def main():
    print("ğŸš€ [Start] Historical BERT Comparison Framework")
    print(f"ğŸ“‚ Project Root: {Config.BASE_DIR}")
    pipelines = {}
    p1 = ModelHandler.load_pipeline(Config.SILLOK_MODEL_PATH, "SillokBERT", Config.TOP_K)
    if p1: pipelines["SillokBERT"] = p1
    p2 = ModelHandler.load_pipeline(Config.SIKU_MODEL_PATH, "SikuRoBERTa", Config.TOP_K)
    if p2: pipelines["SikuRoBERTa"] = p2
    p3 = ModelHandler.load_pipeline(Config.HUE_MODEL_PATH, "HUE (Local)", Config.TOP_K)
    if p3: pipelines["HUE (Local)"] = p3
    if not pipelines:
        logger.error("âŒ ë¡œë“œëœ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    raw_lines = DataProcessor.load_and_clean(Config.TEST_DATA_PATH)
    if not raw_lines:
        return
    test_dataset = DataProcessor.create_masked_dataset(raw_lines, Config.NUM_SAMPLES)
    if not test_dataset:
        logger.error("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨.")
        return
    print("\n" + "#"*60)
    print("ğŸ“Š ì •ëŸ‰ í‰ê°€ ê²°ê³¼ (Quantitative Evaluation)")
    print("#"*60)
    for name, pipe in pipelines.items():
        res = Evaluator.evaluate_quantitative(name, pipe, test_dataset, Config.TOP_K)
        print(f"\nğŸ·ï¸  Model: {name}")
        print(f"   - Top-1 Accuracy: {res['acc_top1']:.2f}%")
        print(f"   - Top-{Config.TOP_K} Accuracy: {res['acc_topk']:.2f}%")
        print(f"   - Valid Samples : {res['count']}")
    if test_dataset:
        Evaluator.compare_qualitative(test_dataset[0], pipelines, Config.TOP_K)
    print("\nğŸ‰ ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
'''
        }
    },
    {
        "path": "nlp/sikuroberta/íŒë…ë¬¸_ì „ì²´_í…ìŠ¤íŠ¸_ì¤‘ë³µ_íƒë³¸_ì œì™¸",
        "title": "íŒë…ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ ì¤‘ë³µ íƒë³¸ ì œì™¸",
        "purpose": "ì¡°ì„ ì™•ì¡°ì‹¤ë¡ íŒë…ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ íƒë³¸ì„ ì œì™¸í•œ ë°ì´í„°ì…‹ êµ¬ì¶•",
        "summary": "í† í¬ë‚˜ì´ì € Vocab í™•ì¥ í›„ í† í°í™”ì™€ ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰í•˜ì—¬ ì¤‘ë³µ ì œê±° ì‹œ ì„±ëŠ¥ì„ í‰ê°€",
        "files": {
            "preprocess.py": r'''import os
import argparse
from transformers import AutoTokenizer
from datasets import load_dataset

def expand_vocab(tokenizer, data_path):
    """
    ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë¬¸ìë¥¼ í™•ì¸í•˜ê³ , ê¸°ì¡´ í† í¬ë‚˜ì´ì € vocabì— ì—†ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print(f"[*] Vocab í™•ì¥ì„ ìœ„í•œ ë°ì´í„° ìŠ¤ìº” ì¤‘: {data_path}")
    unique_chars_in_data = set()
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                unique_chars_in_data.update(list(line.strip()))
    except FileNotFoundError:
        raise FileNotFoundError(f"ì˜¤ë¥˜: {data_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    existing_vocab = set(tokenizer.get_vocab().keys())
    new_tokens = list(unique_chars_in_data - existing_vocab)
    if new_tokens:
        tokenizer.add_tokens(new_tokens)
        print(f"[+] Vocab í™•ì¥ ì™„ë£Œ. ì¶”ê°€ëœ í† í° ìˆ˜: {len(new_tokens)}")
        print(f"[+] ìµœì¢… Vocab í¬ê¸°: {len(tokenizer)}")
    else:
        print("[=] ëª¨ë“  ë¬¸ìê°€ ì´ë¯¸ Vocabì— ì¡´ì¬í•©ë‹ˆë‹¤.")
    return tokenizer

def preprocess_data(args):
    print(f"[*] í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {args.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    tokenizer = expand_vocab(tokenizer, args.input_file)
    print("[*] ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset('text', data_files={'train': args.input_file})
    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=args.max_length,
            padding='max_length',
            return_special_tokens_mask=True
        )
    print("[*] í† í¬ë‚˜ì´ì§• ì§„í–‰ ì¤‘...")
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=['text']
    )
    os.makedirs(args.output_dir, exist_ok=True)
    tokenized_dataset.save_to_disk(args.output_dir)
    print(f"[+] ì „ì²˜ë¦¬ ì™„ë£Œ. ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ: {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SikuRoBERTa Data Preprocessing")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta", help="HuggingFace ëª¨ë¸ëª…")
    parser.add_argument("--input_file", type=str, default="./data/preprocess_txt.txt", help="í•™ìŠµí•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./data/tokenized_dataset", help="í† í°í™”ëœ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--max_length", type=int, default=512, help="ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´")
    args = parser.parse_args()
    preprocess_data(args)
''',
            "train.py": r'''import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer, 
    BertForMaskedLM, 
    DataCollatorForLanguageModeling,
    Trainer, 
    TrainingArguments, 
    EarlyStoppingCallback
)
from datasets import load_from_disk, DatasetDict

def load_tokenizer_and_model(model_name, data_file_path):
    print(f"[*] í† í¬ë‚˜ì´ì € ë¡œë“œ (use_fast=False): {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    unique_chars_in_data = set()
    with open(data_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            unique_chars_in_data.update(list(line.strip()))
    existing_vocab = set(tokenizer.get_vocab().keys())
    new_tokens = list(unique_chars_in_data - existing_vocab)
    if new_tokens:
        tokenizer.add_tokens(new_tokens)
        print(f"[+] Vocab í™•ì¥ ì ìš© ì™„ë£Œ ({len(new_tokens)} ê°œ ì¶”ê°€)")
    print("[*] MaskedLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = BertForMaskedLM.from_pretrained(model_name)
    model.resize_token_embeddings(len(tokenizer))
    return tokenizer, model

def plot_loss_graph(log_history, save_path):
    logs_df = pd.DataFrame(log_history)
    train_logs = logs_df[logs_df["loss"].notna()]
    eval_logs = logs_df[logs_df["eval_loss"].notna()]
    plt.figure(figsize=(12, 6))
    plt.plot(train_logs["step"], train_logs["loss"], label="Training Loss", color='blue', alpha=0.6)
    plt.plot(eval_logs["step"], eval_logs["eval_loss"], label="Validation Loss", color='red', marker='o', linestyle='--')
    plt.xlabel("Step"); plt.ylabel("Loss"); plt.title("Training Loss vs Validation Loss")
    plt.legend(); plt.grid(True)
    plt.savefig(save_path, dpi=200, bbox_inches="tight")
    print(f"[+] í•™ìŠµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")

def train_model(args):
    tokenizer, model = load_tokenizer_and_model(args.model_name, args.raw_data_path)
    try:
        loaded_dataset = load_from_disk(args.dataset_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset_path}. preprocess.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    if 'validation' not in loaded_dataset.keys():
        print("[*] ë°ì´í„°ì…‹ ë¶„í•  ì‹œì‘ (8:1:1)...")
        train_testval = loaded_dataset['train'].train_test_split(test_size=0.2, seed=42)
        test_val = train_testval['test'].train_test_split(test_size=0.5, seed=42)
        tokenized_dataset = DatasetDict({
            'train': train_testval['train'],
            'validation': test_val['train'],
            'test': test_val['test']
        })
    else:
        tokenized_dataset = loaded_dataset
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accum,
        learning_rate=args.learning_rate,
        warmup_ratio=0.06,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        logging_steps=50,
        fp16=True,
        report_to="none"
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['validation'],
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
    )
    print("[*] í•™ìŠµ ì‹œì‘...")
    trainer.train()
    os.makedirs(os.path.dirname(args.graph_path), exist_ok=True)
    plot_loss_graph(trainer.state.log_history, args.graph_path)
    print("[*] ìµœì¢… Test Set í‰ê°€ ì§„í–‰...")
    test_results = trainer.evaluate(tokenized_dataset['test'])
    print(f"[Result] Test Set ê²°ê³¼: {test_results}")
    trainer.save_model(args.final_model_path)
    tokenizer.save_pretrained(args.final_model_path)
    print(f"[+] ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {args.final_model_path}")

def parse_args():
    parser = argparse.ArgumentParser(description="SikuRoBERTa Fine-Tuning")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta")
    parser.add_argument("--raw_data_path", type=str, default="./data/preprocess_txt.txt", help="Vocab í™•ì¥ì„ ìœ„í•œ ì›ë³¸ í…ìŠ¤íŠ¸")
    parser.add_argument("--dataset_path", type=str, default="./data/tokenized_dataset", help="ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./output/checkpoints")
    parser.add_argument("--final_model_path", type=str, default="./output/final_model")
    parser.add_argument("--graph_path", type=str, default="./output/loss_graph.png")
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--gradient_accum", type=int, default=2)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    train_model(args)
'''
        }
    },
    {
        "path": "nlp/sikuroberta/íŒë…ë¬¸_ì „ì²´_í…ìŠ¤íŠ¸_ì¤‘ë³µ_íƒë³¸_í¬í•¨",
        "title": "íŒë…ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸ ì¤‘ë³µ íƒë³¸ í¬í•¨",
        "purpose": "ì¤‘ë³µ íƒë³¸ì„ í¬í•¨í•œ ì „ì²´ íŒë…ë¬¸ ë°ì´í„°ì…‹ìœ¼ë¡œ MLM í•™ìŠµ",
        "summary": "ì¤‘ë³µ ë°ì´í„°ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í† í¬ë‚˜ì´ì§• í›„ MLM í•™ìŠµì„ ì§„í–‰",
        "files": {
            "preprocess.py": r'''import os
import argparse
from itertools import chain
from transformers import AutoTokenizer
from datasets import load_dataset

def expand_vocab(tokenizer, data_path):
    print(f"[*] Scanning data for vocabulary expansion: {data_path}")
    unique_chars_in_data = set()
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                unique_chars_in_data.update(list(line.strip()))
    except FileNotFoundError:
        raise FileNotFoundError(f"Error: File {data_path} not found.")
    existing_vocab = set(tokenizer.get_vocab().keys())
    new_tokens = list(unique_chars_in_data - existing_vocab)
    if new_tokens:
        tokenizer.add_tokens(new_tokens)
        print(f"[+] Added {len(new_tokens)} new tokens to vocabulary.")
    else:
        print("[=] No new tokens needed. Vocabulary is complete.")
    return tokenizer

def preprocess_and_group(args):
    print(f"[*] Loading tokenizer: {args.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    tokenizer = expand_vocab(tokenizer, args.input_file)
    print("[*] Loading text dataset...")
    dataset = load_dataset('text', data_files={'train': args.input_file})
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            return_special_tokens_mask=True
        )
    print("[*] Tokenizing raw text...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"]
    )
    def group_texts(examples):
        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        if total_length >= args.block_size:
            total_length = (total_length // args.block_size) * args.block_size
        result = {
            k: [t[i : i + args.block_size] for i in range(0, total_length, args.block_size)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result
    print(f"[*] Grouping text into chunks of {args.block_size} tokens...")
    lm_datasets = tokenized_datasets.map(
        group_texts,
        batched=True,
        batch_size=1000,
        num_proc=args.num_proc
    )
    print(f"[+] Original count: {len(dataset['train'])}")
    print(f"[+] Grouped count : {len(lm_datasets['train'])}")
    lm_datasets.save_to_disk(args.output_dir)
    print(f"[+] Saved processed dataset to: {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SikuRoBERTa Data Grouping & Preprocessing")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta", help="HuggingFace model name")
    parser.add_argument("--input_file", type=str, required=True, help="Path to raw .txt file")
    parser.add_argument("--output_dir", type=str, default="./data/processed_dataset", help="Path to save tokenized data")
    parser.add_argument("--block_size", type=int, default=256, help="Token block size (chunk size)")
    parser.add_argument("--num_proc", type=int, default=4, help="Number of CPU processes for mapping")
    args = parser.parse_args()
    preprocess_and_group(args)
''',
            "train.py": r'''import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer, 
    BertForMaskedLM, 
    DataCollatorForLanguageModeling,
    Trainer, 
    TrainingArguments, 
    EarlyStoppingCallback
)
from datasets import load_from_disk, DatasetDict

def load_resources(args):
    print(f"[*] Loading tokenizer: {args.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    unique_chars_in_data = set()
    if os.path.exists(args.raw_data_path):
        with open(args.raw_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                unique_chars_in_data.update(list(line.strip()))
        existing_vocab = set(tokenizer.get_vocab().keys())
        new_tokens = list(unique_chars_in_data - existing_vocab)
        if new_tokens:
            tokenizer.add_tokens(new_tokens)
            print(f"[+] Vocab expanded by {len(new_tokens)} tokens.")
    else:
        print("[!] Warning: Raw data path not found. Skipping Vocab check.")
    print("[*] Loading Model (BertForMaskedLM)...")
    model = BertForMaskedLM.from_pretrained(args.model_name)
    model.resize_token_embeddings(len(tokenizer))
    return tokenizer, model

def prepare_dataset(dataset_path):
    print(f"[*] Loading dataset from: {dataset_path}")
    try:
        loaded_dataset = load_from_disk(dataset_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"Dataset not found at {dataset_path}. Run preprocess.py first.")
    if 'validation' not in loaded_dataset.keys():
        print("[*] Splitting dataset (80% Train, 10% Valid, 10% Test)...")
        train_testval = loaded_dataset['train'].train_test_split(test_size=0.2, seed=42)
        test_val = train_testval['test'].train_test_split(test_size=0.5, seed=42)
        return DatasetDict({
            'train': train_testval['train'],
            'validation': test_val['train'],
            'test': test_val['test']
        })
    return loaded_dataset

def plot_training_history(log_history, save_path):
    logs_df = pd.DataFrame(log_history)
    train_logs = logs_df[logs_df["loss"].notna()]
    eval_logs = logs_df[logs_df["eval_loss"].notna()]
    plt.figure(figsize=(12, 6))
    plt.plot(train_logs["step"], train_logs["loss"], label="Training Loss", color='blue', alpha=0.6)
    plt.plot(eval_logs["step"], eval_logs["eval_loss"], label="Validation Loss", color='red', marker='o', linestyle='--')
    plt.xlabel("Step"); plt.ylabel("Loss"); plt.title("Training Loss vs Validation Loss")
    plt.legend(); plt.grid(True)
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=200, bbox_inches="tight")
    print(f"[+] Loss graph saved to: {save_path}")

def main(args):
    tokenizer, model = load_resources(args)
    tokenized_dataset = prepare_dataset(args.dataset_path)
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.learning_rate,
        warmup_ratio=0.1,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        logging_steps=50,
        fp16=True,
        report_to="none"
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['validation'],
        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.patience)]
    )
    print("[*] Starting training...")
    trainer.train()
    plot_training_history(trainer.state.log_history, args.graph_path)
    print("[*] Evaluating on Test Set...")
    test_results = trainer.evaluate(tokenized_dataset['test'])
    print(f"[Result] {test_results}")
    trainer.save_model(args.final_model_dir)
    tokenizer.save_pretrained(args.final_model_dir)
    print(f"[+] Final model saved to: {args.final_model_dir}")

def parse_args():
    parser = argparse.ArgumentParser(description="SikuRoBERTa Training Script")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta")
    parser.add_argument("--raw_data_path", type=str, required=True, help="Path to raw txt for vocab check")
    parser.add_argument("--dataset_path", type=str, default="./data/processed_dataset", help="Path to processed (grouped) dataset")
    parser.add_argument("--output_dir", type=str, default="./output/checkpoints")
    parser.add_argument("--final_model_dir", type=str, default="./output/final_model")
    parser.add_argument("--graph_path", type=str, default="./output/loss_graph.png")
    parser.add_argument("--epochs", type=int, default=30)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--grad_accum", type=int, default=2)
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--patience", type=int, default=3, help="Early stopping patience")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    main(args)
'''
        }
    },
    {
        "path": "nlp/sikuroberta/ë¬¸ì¥_í•™ìŠµ",
        "title": "ë¬¸ì¥ í•™ìŠµ ì½”ë“œ ë°±ì—…",
        "purpose": "ë¬¸ì¥ ë‹¨ìœ„ì˜ í•œë¬¸ ë§ë­‰ì¹˜ë¡œ SikuRoBERTaë¥¼ í•™ìŠµí•˜ëŠ” ì‹¤í—˜ ì½”ë“œ ë³´ê´€",
        "summary": "Dynamic Padding ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë³„ ê¸¸ì´ì— ë§ì¶˜ í•™ìŠµì„ ìˆ˜í–‰",
        "files": {
            "utils.py": r'''import os
from transformers import AutoTokenizer

def load_tokenizer_and_expand_vocab(model_name, data_path, use_fast=False):
    print(f"[*] í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)
    print(f"[*] Vocab í™•ì¥ì„ ìœ„í•œ ë°ì´í„° ìŠ¤ìº” ì¤‘: {data_path}")
    unique_chars_in_data = set()
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                unique_chars_in_data.update(list(line.strip()))
    except FileNotFoundError:
        raise FileNotFoundError(f"ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ {data_path}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    existing_vocab = set(tokenizer.get_vocab().keys())
    new_tokens = list(unique_chars_in_data - existing_vocab)
    if new_tokens:
        tokenizer.add_tokens(new_tokens)
        print(f"[+] Vocab í™•ì¥ ì™„ë£Œ. ì¶”ê°€ëœ í† í° ìˆ˜: {len(new_tokens)}")
    else:
        print("[=] ëª¨ë“  ë¬¸ìê°€ ì´ë¯¸ Vocabì— ì¡´ì¬í•©ë‹ˆë‹¤.")
    return tokenizer
''',
            "preprocess.py": r'''import os
import argparse
from datasets import load_dataset
from utils import load_tokenizer_and_expand_vocab

def preprocess_dynamic(args):
    tokenizer = load_tokenizer_and_expand_vocab(args.model_name, args.input_file)
    print("[*] ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset('text', data_files={'train': args.input_file})
    print(f"[Info] í•„í„°ë§ ì „ ë°ì´í„° ê°œìˆ˜: {len(dataset['train'])}")
    dataset = dataset.filter(lambda example: len(example['text']) >= 10)
    print(f"[Info] í•„í„°ë§ í›„(10ì ì´ìƒ) ë°ì´í„° ê°œìˆ˜: {len(dataset['train'])}")
    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=args.max_length,
            padding=False,
            return_special_tokens_mask=True
        )
    print("[*] í† í¬ë‚˜ì´ì§• ì§„í–‰ (padding=False ì ìš©)...")
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=['text']
    )
    print("\n[Check] ìƒ˜í”Œ ë°ì´í„° ê¸¸ì´ í™•ì¸ (ê¸¸ì´ê°€ ì œê°ê°ì´ì–´ì•¼ ì •ìƒ):")
    for i in range(min(3, len(tokenized_dataset['train']))):
        print(f" - Sample {i} length: {len(tokenized_dataset['train'][i]['input_ids'])}")
    os.makedirs(args.output_dir, exist_ok=True)
    tokenized_dataset.save_to_disk(args.output_dir)
    print(f"[+] ì „ì²˜ë¦¬ ì™„ë£Œ. ì €ì¥ ê²½ë¡œ: {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SikuRoBERTa Dynamic Padding Preprocessing")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta")
    parser.add_argument("--input_file", type=str, required=True, help="ì›ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./data/processed_dynamic", help="ì €ì¥í•  ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--max_length", type=int, default=128, help="ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´")
    args = parser.parse_args()
    preprocess_dynamic(args)
''',
            "train.py": r'''import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from transformers import (
    BertForMaskedLM, 
    DataCollatorForLanguageModeling,
    Trainer, 
    TrainingArguments, 
    EarlyStoppingCallback
)
from transformers.trainer_utils import get_last_checkpoint
from datasets import load_from_disk, DatasetDict
from utils import load_tokenizer_and_expand_vocab

def train_model(args):
    tokenizer = load_tokenizer_and_expand_vocab(args.model_name, args.raw_data_path)
    print("[*] ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ë¦¬ì‚¬ì´ì§•...")
    model = BertForMaskedLM.from_pretrained(args.model_name)
    model.resize_token_embeddings(len(tokenizer))
    print(f"[*] ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ë¡œë“œ: {args.dataset_path}")
    try:
        loaded_dataset = load_from_disk(args.dataset_path)
    except FileNotFoundError:
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ì—†ìŒ: {args.dataset_path}. preprocess.py ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    if 'validation' not in loaded_dataset.keys():
        print("[*] ë°ì´í„° ë¶„í•  ì‹œì‘ (Train 80% / Valid 10% / Test 10%)...")
        train_testval = loaded_dataset['train'].train_test_split(test_size=0.2, seed=42)
        test_val = train_testval['test'].train_test_split(test_size=0.5, seed=42)
        tokenized_dataset = DatasetDict({
            'train': train_testval['train'],
            'validation': test_val['train'],
            'test': test_val['test']
        })
    else:
        tokenized_dataset = loaded_dataset
    if args.save_split_dataset:
        split_save_path = args.dataset_path + "_split"
        tokenized_dataset.save_to_disk(split_save_path)
        print(f"[Info] ë¶„í• ëœ ë°ì´í„°ì…‹ ë³„ë„ ì €ì¥ë¨: {split_save_path}")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True,
        group_by_length=True,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.learning_rate,
        weight_decay=0.01,
        warmup_ratio=0.06,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        logging_steps=100,
        fp16=True,
        report_to="none"
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset['train'],
        eval_dataset=tokenized_dataset['validation'],
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    print("[*] í•™ìŠµ ì‹œì‘...")
    last_checkpoint = get_last_checkpoint(args.output_dir)
    if last_checkpoint:
        print(f"[Resume] ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {last_checkpoint} ì—ì„œ ì¬ê°œí•©ë‹ˆë‹¤.")
        trainer.train(resume_from_checkpoint=last_checkpoint)
    else:
        trainer.train()
    def plot_loss_graph(log_history, save_path):
        logs_df = pd.DataFrame(log_history)
        train_logs = logs_df[logs_df["loss"].notna()]
        eval_logs = logs_df[logs_df["eval_loss"].notna()]
        plt.figure(figsize=(10, 6))
        plt.plot(train_logs["epoch"], train_logs["loss"], label="Training Loss", color='blue', alpha=0.6)
        plt.plot(eval_logs["epoch"], eval_logs["eval_loss"], label="Validation Loss", color='red', marker='o', linestyle='--')
        plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Training vs Validation Loss (Dynamic Padding)")
        plt.legend(); plt.grid(True)
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=200, bbox_inches="tight")
        print(f"[+] ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    plot_loss_graph(trainer.state.log_history, args.graph_path)
    trainer.save_model(args.final_model_dir)
    tokenizer.save_pretrained(args.final_model_dir)
    print(f"[+] ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {args.final_model_dir}")

def parse_args():
    parser = argparse.ArgumentParser(description="SikuRoBERTa Dynamic Padding Training")
    parser.add_argument("--model_name", type=str, default="SIKU-BERT/sikuroberta")
    parser.add_argument("--raw_data_path", type=str, required=True, help="Vocab í™•ì¥ì„ ìœ„í•œ ì›ë³¸ í…ìŠ¤íŠ¸")
    parser.add_argument("--dataset_path", type=str, required=True, help="ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="./output/checkpoints")
    parser.add_argument("--final_model_dir", type=str, default="./output/final_model")
    parser.add_argument("--graph_path", type=str, default="./output/loss_graph.png")
    parser.add_argument("--save_split_dataset", action="store_true", help="ë¶„í• ëœ ë°ì´í„°ì…‹ì„ ë³„ë„ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€")
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--grad_accum", type=int, default=2)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    train_model(args)
'''
        }
    },
    {
        "path": "nlp/gemini/ExaOne_ì„±ëŠ¥_í‰ê°€",
        "title": "ExaOne ì„±ëŠ¥ í‰ê°€ ì½”ë“œ",
        "purpose": "ëŒ€ê·œëª¨ LLMì¸ ExaOne-3.5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œë¬¸ ê¸ˆì„ë¬¸ì˜ ë²ˆì—­ ì„±ëŠ¥ì„ ì¸¡ì •",
        "summary": "ìŒë…, ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ, ë²ˆì—­ì„ ëª¨ë‘ ìˆ˜í–‰í•˜ê³  Kiwi ê¸°ë°˜ BLEU í‰ê°€ë¡œ ê²°ê³¼ë¥¼ ë¶„ì„",
        "files": {
            "translator.py": r'''# -*- coding: utf-8 -*-
"""
Hanja Translation Script using EXAONE Model
Author: [Your Name]
Description: í•œë¬¸ ê¸ˆì„ë¬¸ì„ ì…ë ¥ë°›ì•„ ìŒë…, ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ, êµ­ì—­ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""
import os
import re
import pandas as pd
import torch
from kiwipiepy import Kiwi
import sacrebleu
from transformers import AutoModelForCausalLM, AutoTokenizer

class Config:
    MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    INPUT_CSV_PATH = "../data/pun_ksm_gsko_filtered.csv"
    OUTPUT_CSV_PATH = "../data/translation_results_exaone_filtered.csv"
    TARGET_COUNT = 1000
    BATCH_SIZE = 50
    MAX_NEW_TOKENS = 600
    REPETITION_PENALTY = 1.2
    SYSTEM_PROMPT = """<role>
ë‹¹ì‹ ì€ í•œë¬¸ ê¸ˆì„ë¬¸ ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
</role>

<task>
ì…ë ¥ëœ í•œì ì›ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë°˜ë“œì‹œ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤:
1. [ìŒë…]: ì›ë¬¸ì˜ ëª¨ë“  í•œìë¥¼ ë„ì–´ì“°ê¸° ì—†ì´ ì •í™•í•œ í•œê¸€ ë…ìŒìœ¼ë¡œë§Œ ë³€í™˜
2. [ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ]: ì¸ëª…, ê´€ì§, ì§€ëª…, ì—°í˜¸ë¥¼ 'í•œê¸€(í•œì)' í˜•íƒœë¡œ ë‚˜ì—´
3. [ìµœì¢… ë²ˆì—­]: ìœ„ ê³ ìœ ëª…ì‚¬ë¥¼ í™œìš©í•˜ì—¬ ì˜ˆìŠ¤ëŸ¬ìš´ ë¬¸ì–´ì²´(~í•˜ë‹ˆë¼, ~í•˜ë‹¤)ë¡œ ì§ì—­
</task>

<constraints>
- ì›ë¬¸ì— í‘œê¸°ë˜ì§€ ì•Šì€ ê¸€ìë¥¼ ì„ì˜ë¡œ ìƒì„±í•˜ê±°ë‚˜ ìœ ì¶”í•˜ì—¬ í•´ì„ì— í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤
- ì›ë¬¸ì— ìˆëŠ” ê²°ë½ ê¸°í˜¸(â–¨)ëŠ” ìƒëµí•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë°˜ë“œì‹œ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ â–¨ ê¸°í˜¸ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ë²ˆì—­ë¬¸ì— í¬í•¨í•˜ì‹­ì‹œì˜¤.
- ì„¤ëª…ì´ë‚˜ ì‚¬ì¡±ì„ ë§ë¶™ì´ì§€ ë§ˆì‹­ì‹œì˜¤
- ì •í™•í•œ í˜•ì‹ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì‹­ì‹œì˜¤
</constraints>

<output_format>
[ìŒë…]: (í•œê¸€ ìŒë…)
[ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ]: (ì¶”ì¶œëœ ê³ ìœ ëª…ì‚¬ë“¤)
[ìµœì¢… ë²ˆì—­]: (ìµœì¢… ë²ˆì—­ë¬¸)
</output_format>"""
    FEW_SHOT_EXAMPLES = """<examples>
<example>
<input>åºœå°¹åš´ç›¸å…¬å–„æ”¿ç¢‘ å…¬è«±é¼è€‰ å­—é‡å” æ­²æˆŠæˆŒå¤å››æœˆä¸‹è»Š å·±äº¥å†¬åæœˆä»¥ç—…è¾­æ­¸</input>
<output>
[ìŒë…]: ë¶€ìœ¤ì—„ìƒê³µì„ ì •ë¹„ê³µíœ˜ì •êµ¬ìì¤‘ìˆ™ì„¸ë¬´ìˆ í•˜ì‚¬ì›”í•˜ì°¨ê¸°í•´ë™ì‹œì›”ì´ë³‘ì‚¬ê·€
[ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ]: ë¶€ìœ¤(åºœå°¹), ì—„ìƒê³µ(åš´ç›¸å…¬), ì„ ì •ë¹„(å–„æ”¿ç¢‘), íœ˜(è«±), ì •êµ¬(é¼è€‰), ì(å­—), ì¤‘ìˆ™(é‡å”), ë¬´ìˆ ë…„(æˆŠæˆŒ), ê¸°í•´ë…„(å·±äº¥)
[ìµœì¢… ë²ˆì—­]: ë¶€ìœ¤(åºœå°¹) ì—„ ìƒê³µ(åš´ç›¸å…¬)ì˜ ì„ ì •ë¹„(å–„æ”¿ç¢‘). ê³µì˜ íœ˜ëŠ” ì •êµ¬(é¼è€‰)ì´ê³ , ìëŠ” ì¤‘ìˆ™(é‡å”)ì´ë‹¤. ë¬´ìˆ ë…„(æˆŠæˆŒ) ì—¬ë¦„ 4ì›”ì— ë¶€ì„í•˜ì˜€ê³ , ê¸°í•´ë…„(å·±äº¥) ê²¨ìš¸ 10ì›”ì— ë³‘ìœ¼ë¡œ ì‚¬ì§í•˜ê³  ëŒì•„ê°”ë‹¤.
</output>
</example>
</examples>"""

class TextUtils:
    def __init__(self):
        print("âš™ï¸ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.kiwi = Kiwi()
    def get_nouns_only(self, text: str) -> str:
        if not isinstance(text, str):
            return ""
        text = re.sub(r'\([^)]*\)', ' ', text)
        text = re.sub(r'[^ê°€-í£\s]', '', text)
        try:
            tokens = self.kiwi.tokenize(text)
            targets = {'NNG', 'NNP', 'NR', 'NP', 'NNB'}
            nouns = [t.form for t in tokens if t.tag in targets]
            return " ".join(nouns)
        except Exception:
            return ""
    @staticmethod
    def calculate_bleu(reference: str, hypothesis: str, noun_extractor_func) -> tuple:
        ref_nouns = noun_extractor_func(reference)
        hyp_nouns = noun_extractor_func(hypothesis)
        if not ref_nouns or not hyp_nouns:
            return 0.0, ref_nouns, hyp_nouns
        bleu = sacrebleu.sentence_bleu(hyp_nouns, [ref_nouns], tokenize='char')
        return bleu.score, ref_nouns, hyp_nouns
    @staticmethod
    def extract_translation_part(full_text: str) -> str:
        markers = ["[ìµœì¢… ë²ˆì—­]:", "[ìµœì¢… ë²ˆì—­]", "[ë²ˆì—­]:"]
        for marker in markers:
            if marker in full_text:
                return full_text.split(marker)[1].strip()
        lines = full_text.split('\n')
        return lines[-1] if lines else full_text

class HanjaTranslator:
    def __init__(self, config: Config):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self._load_model()
    def _load_model(self):
        print(f"âš™ï¸ ëª¨ë¸ ë¡œë”© ì¤‘... ({self.config.MODEL_NAME}) on {self.device}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.MODEL_NAME,
                torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True,
                device_map="auto"
            )
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            raise RuntimeError(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    def translate(self, input_text: str) -> str:
        messages = [
            {"role": "system", "content": self.config.SYSTEM_PROMPT},
            {"role": "user", "content": f"{self.config.FEW_SHOT_EXAMPLES}\n\n### ë¬¸ì œ\nì›ë¬¸: {input_text}\në°˜ë“œì‹œ ìœ„ ì˜ˆì‹œì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n[ìŒë…]:"}
        ]
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            output = self.model.generate(
                input_ids.to(self.device),
                eos_token_id=self.tokenizer.eos_token_id,
                max_new_tokens=self.config.MAX_NEW_TOKENS,
                do_sample=False,
                repetition_penalty=self.config.REPETITION_PENALTY
            )
            input_length = input_ids.shape[1]
            generated_tokens = output[0][input_length:]
            result = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            if not result.startswith("[ìŒë…]"):
                result = "[ìŒë…]: " + result
            return result
        except Exception as e:
            return f"Error: {str(e)}"

def save_results(data: list, path: str):
    df = pd.DataFrame(data)
    if not os.path.exists(path):
        df.to_csv(path, index=False, encoding='utf-8-sig', mode='w')
    else:
        df.to_csv(path, index=False, encoding='utf-8-sig', mode='a', header=False)
    print(f"ğŸ’¾ {len(data)}ê±´ ì €ì¥ ì™„ë£Œ")

def main():
    config = Config()
    utils = TextUtils()
    if not os.path.exists(config.INPUT_CSV_PATH):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.INPUT_CSV_PATH}")
        return
    df = pd.read_csv(config.INPUT_CSV_PATH)
    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê±´")
    translator = HanjaTranslator(config)
    actual_sample_size = min(config.TARGET_COUNT, len(df))
    target_df = df.sample(n=actual_sample_size, random_state=42).copy()
    print(f"ğŸš€ {actual_sample_size}ê°œ ë°ì´í„° ë²ˆì—­ ì‹œì‘...")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {config.OUTPUT_CSV_PATH}\n")
    results_buffer = []
    for i, (idx, row) in enumerate(target_df.iterrows(), 1):
        src = row.get('pun_transcription', '')
        ref = row.get('translation', '')
        if pd.isna(src) or str(src).strip() == "":
            continue
        full_output = translator.translate(src)
        if "Error" in full_output:
            print(f"âš ï¸ [Skip] {idx}ë²ˆ ì—ëŸ¬: {full_output}")
            continue
        hyp_clean = utils.extract_translation_part(full_output)
        score, ref_nouns, hyp_nouns = utils.calculate_bleu(ref, hyp_clean, utils.get_nouns_only)
        results_buffer.append({
            'original_index': idx,
            'src_hanja': src,
            'ref_korean': ref,
            'hyp_full': full_output,
            'hyp_clean': hyp_clean,
            'ref_nouns': ref_nouns,
            'hyp_nouns': hyp_nouns,
            'bleu_score': score
        })
        print(f"[{i}/{actual_sample_size}] BLEU: {score:.2f} | ì •ë‹µ: {ref_nouns[:10]}... | ì˜ˆì¸¡: {hyp_nouns[:10]}...")
        if len(results_buffer) >= config.BATCH_SIZE:
            save_results(results_buffer, config.OUTPUT_CSV_PATH)
            results_buffer = []
    if results_buffer:
        save_results(results_buffer, config.OUTPUT_CSV_PATH)
    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
''',
            "analyzer.py": r'''# -*- coding: utf-8 -*-
"""
Translation Result Analyzer
Author: [Your Name]
Description: ë²ˆì—­ ê²°ê³¼ CSV íŒŒì¼ì„ ì½ì–´ BLEU ì ìˆ˜(Corpus & Sentence)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import os
import pandas as pd
import sacrebleu
RESULT_CSV_PATH = "../data/translation_results_exaone_filtered.csv"

def analyze_translation_results(file_path: str):
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return
    df = pd.read_csv(file_path)
    valid_df = df.dropna(subset=['ref_nouns', 'hyp_nouns'])
    print("=" * 50)
    print(f"ğŸ“Š ë²ˆì—­ í’ˆì§ˆ ë¶„ì„ ë³´ê³ ì„œ")
    print("=" * 50)
    print(f"â€¢ ì „ì²´ ë°ì´í„°: {len(df)}ê±´")
    print(f"â€¢ ìœ íš¨ ë°ì´í„°: {len(valid_df)}ê±´ (ëˆ„ë½ ë°ì´í„° ì œì™¸)")
    print("-" * 50)
    if len(valid_df) == 0:
        print("âš ï¸ ìœ íš¨í•œ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    avg_sentence_bleu = valid_df['bleu_score'].mean()
    refs = valid_df['ref_nouns'].tolist()
    preds = valid_df['hyp_nouns'].tolist()
    corpus_bleu = sacrebleu.corpus_bleu(preds, [refs], tokenize='char')
    print(f"âœ… 1. Sentence BLEU í‰ê·  : {avg_sentence_bleu:.2f}")
    print(f"ğŸ† 2. Corpus BLEU Score   : {corpus_bleu.score:.2f}")
    print("=" * 50)

if __name__ == "__main__":
    analyze_translation_results(RESULT_CSV_PATH)
'''
        }
    },
    {
        "path": "nlp/gemini/Qwen_ì„±ëŠ¥_í‰ê°€",
        "title": "Qwen ì„±ëŠ¥ í‰ê°€ ì½”ë“œ",
        "purpose": "Qwen ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œë¬¸ ë²ˆì—­ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³  í‰ê°€",
        "summary": "ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ê³„í•˜ì—¬ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ê³  BLEU í‰ê°€ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •",
        "files": {
            "translator.py": r'''# -*- coding: utf-8 -*-
"""
Hanja Translation Script using Qwen Model
Author: [Your Name]
Description: Qwen ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í•œë¬¸ ê¸ˆì„ë¬¸ì„ ê²©ì‹ ìˆëŠ” êµ­ë¬¸ìœ¼ë¡œ ë²ˆì—­í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""
import os
import re
import pandas as pd
import torch
from kiwipiepy import Kiwi
import sacrebleu
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

class Config:
    MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
    INPUT_CSV_PATH = "../data/pun_ksm_gsko_filtered.csv"
    OUTPUT_CSV_PATH = "../data/qwen_translation_results.csv"
    TARGET_COUNT = 750
    BATCH_SIZE = 50
    MAX_NEW_TOKENS = 600
    REPETITION_PENALTY = 1.1
    SYSTEM_PROMPT = """<role>
ë‹¹ì‹ ì€ ê³ ì „ í•œë¬¸ ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
</role>

<task>
ì£¼ì–´ì§„ í•œë¬¸ì„ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ê²©ì‹ ìˆëŠ” í•œêµ­ì–´ ë¬¸ì–´ì²´(ì˜ˆ: ~í•˜ë‹ˆë¼, ~í•˜ì˜€ë”ë¼, ~ì´ë‹ˆë¼)ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤.
</task>

<constraints>
1. ë¶€ê°€ì ì¸ ì„¤ëª…, ì£¼ì„, ë°œìŒ(ìŒë…), ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ ëª©ë¡ ë“±ì„ **ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**
2. ì˜¤ì§ **ìµœì¢… ë²ˆì—­ë¬¸ë§Œ** ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
3. ì›ë¬¸ì— ìˆëŠ” ê²°ë½ ê¸°í˜¸(â–¨)ëŠ” ìƒëµí•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ê°œìˆ˜ë¥¼ ìœ ì§€í•˜ì—¬ ë²ˆì—­ë¬¸ì— ê·¸ëŒ€ë¡œ í¬í•¨í•˜ì‹­ì‹œì˜¤.
4. ë¬¸ì²´ëŠ” ì˜›ìŠ¤ëŸ¬ìš´ ì–´ì¡°(~í•˜ë‹ˆë¼, ~í•˜ë”ë¼)ë¥¼ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
</constraints>"""
    FEW_SHOT_EXAMPLES = """<examples>
<example>
<input>åºœå°¹åš´ç›¸å…¬å–„æ”¿ç¢‘ å…¬è«±é¼è€‰ å­—é‡å” æ­²æˆŠæˆŒå¤å››æœˆä¸‹è»Š å·±äº¥å†¬åæœˆä»¥ç—…è¾­æ­¸</input>
<output>
ë¶€ìœ¤ ì—„ ìƒê³µì˜ ì„ ì •ë¹„ë¼. ê³µì˜ íœ˜ëŠ” ì •êµ¬ìš” ìëŠ” ì¤‘ìˆ™ì´ë‹ˆ, ë¬´ìˆ ë…„ ì—¬ë¦„ 4ì›”ì— ë¶€ì„í•˜ì—¬ ê¸°í•´ë…„ ê²¨ìš¸ 10ì›”ì— ë³‘ìœ¼ë¡œ ì‚¬ì§í•˜ê³  ëŒì•„ê°”ëŠë‹ˆë¼.
</output>
</example>
</examples>"""

class TextUtils:
    def __init__(self):
        print("âš™ï¸ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.kiwi = Kiwi()
    def get_nouns_only(self, text: str) -> str:
        if not isinstance(text, str): 
            return ""
        text = re.sub(r'\([^)]*\)', ' ', text)
        text = re.sub(r'[^ê°€-í£\s]', '', text)
        try:
            tokens = self.kiwi.tokenize(text)
            targets = {'NNG', 'NNP', 'NR', 'NP', 'NNB'}
            nouns = [t.form for t in tokens if t.tag in targets]
            return " ".join(nouns)
        except Exception:
            return ""
    def calculate_bleu(self, reference: str, hypothesis: str) -> tuple:
        ref_nouns = self.get_nouns_only(reference)
        hyp_nouns = self.get_nouns_only(hypothesis)
        if not ref_nouns or not hyp_nouns:
            return 0.0, ref_nouns, hyp_nouns
        bleu = sacrebleu.sentence_bleu(hyp_nouns, [ref_nouns], tokenize='char')
        return bleu.score, ref_nouns, hyp_nouns
    @staticmethod
    def extract_translation(full_text: str) -> str:
        clean_text = full_text.strip()
        markers = ["[ìµœì¢… ë²ˆì—­]:", "[ìµœì¢… ë²ˆì—­]", "[Output]:"]
        for marker in markers:
            if marker in clean_text:
                clean_text = clean_text.split(marker)[1].strip()
                break
        return clean_text

class QwenTranslator:
    def __init__(self, config: Config):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self._load_model()
    def _load_model(self):
        print(f"âš™ï¸ ëª¨ë¸ ë¡œë”© ì¤‘... ({self.config.MODEL_NAME}) on {self.device}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.MODEL_NAME,
                torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            raise RuntimeError(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    def translate(self, text: str) -> str:
        messages = [
            {"role": "system", "content": self.config.SYSTEM_PROMPT},
            {"role": "user", "content": f"{self.config.FEW_SHOT_EXAMPLES}\n\n### ë¬¸ì œ\nì›ë¬¸: {text}\nìœ„ ì˜ˆì‹œì™€ ê°™ì´ ì˜¤ì§ ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."}
        ]
        try:
            text_input = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            model_inputs = self.tokenizer([text_input], return_tensors="pt").to(self.device)
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=self.config.MAX_NEW_TOKENS,
                    do_sample=False,
                    repetition_penalty=self.config.REPETITION_PENALTY,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            input_len = model_inputs.input_ids.shape[1]
            generated_ids = generated_ids[0][input_len:]
            result = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            return result
        except Exception as e:
            return f"Error: {str(e)}"

def save_results(data: list, path: str):
    df = pd.DataFrame(data)
    if not os.path.exists(path):
        df.to_csv(path, index=False, encoding='utf-8-sig', mode='w')
    else:
        df.to_csv(path, index=False, encoding='utf-8-sig', mode='a', header=False)


def main():
    config = Config()
    utils = TextUtils()
    if not os.path.exists(config.INPUT_CSV_PATH):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.INPUT_CSV_PATH}")
        return
    df = pd.read_csv(config.INPUT_CSV_PATH)
    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê±´")
    translator = QwenTranslator(config)
    actual_sample_size = min(config.TARGET_COUNT, len(df))
    target_df = df.sample(n=actual_sample_size, random_state=42).copy()
    print(f"\nğŸš€ ë²ˆì—­ ì‹œì‘! (ì´ {actual_sample_size}ê±´)")
    print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {config.OUTPUT_CSV_PATH}\n")
    results_buffer = []
    for i, (idx, row) in enumerate(tqdm(target_df.iterrows(), total=actual_sample_size, desc="Translating"), 1):
        src = row.get('pun_transcription', '')
        ref = row.get('translation', '')
        if pd.isna(src) or str(src).strip() == "":
            continue
        full_output = translator.translate(src)
        if "Error" in full_output:
            continue
        hyp_clean = utils.extract_translation(full_output)
        score, ref_nouns, hyp_nouns = utils.calculate_bleu(ref, hyp_clean)
        results_buffer.append({
            'original_index': idx,
            'src_hanja': src,
            'ref_korean': ref,
            'hyp_full': full_output,
            'hyp_clean': hyp_clean,
            'ref_nouns': ref_nouns,
            'hyp_nouns': hyp_nouns,
            'bleu_score': score
        })
        if len(results_buffer) >= config.BATCH_SIZE:
            save_results(results_buffer, config.OUTPUT_CSV_PATH)
            results_buffer = []
    if results_buffer:
        save_results(results_buffer, config.OUTPUT_CSV_PATH)

if __name__ == "__main__":
    main()
''',
            "analyzer.py": r'''# -*- coding: utf-8 -*-
"""
Translation Result Analyzer (Qwen)
Author: [Your Name]
Description: Qwen ëª¨ë¸ì˜ ë²ˆì—­ ê²°ê³¼ CSVë¥¼ ì½ì–´ BLEU ì ìˆ˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import os
import pandas as pd
import sacrebleu
RESULT_CSV_PATH = "../data/qwen_translation_results.csv"

def analyze_results(file_path: str):
    if not os.path.exists(file_path):
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    valid_df = df.dropna(subset=['ref_nouns', 'hyp_nouns'])
    print("=" * 60)
    print(f"ğŸ“Š Qwen ë²ˆì—­ í’ˆì§ˆ ë¶„ì„ ë³´ê³ ì„œ")
    print("=" * 60)
    print(f"â€¢ ì „ì²´ ë°ì´í„° ìˆ˜ : {len(df)}ê±´")
    print(f"â€¢ ìœ íš¨ í‰ê°€ ë°ì´í„°: {len(valid_df)}ê±´ (ëˆ„ë½ ì œì™¸)")
    print("-" * 60)
    if len(valid_df) == 0:
        print("âš ï¸ í‰ê°€í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    avg_sentence_bleu = valid_df['bleu_score'].mean()
    refs = valid_df['ref_nouns'].tolist()
    preds = valid_df['hyp_nouns'].tolist()
    corpus_bleu = sacrebleu.corpus_bleu(preds, [refs], tokenize='char')
    print(f"âœ… 1. Sentence BLEU í‰ê·  : {avg_sentence_bleu:.2f}")
    print("-" * 40)
    print(f"ğŸ† 2. Corpus BLEU Score   : {corpus_bleu.score:.2f}")
    print("=" * 60)

if __name__ == "__main__":
    analyze_results(RESULT_CSV_PATH)
'''
        }
    },
    {
        "path": "vision/preprocessing/OpenCV_1-6",
        "title": "OpenCV ì´ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ 1-6",
        "purpose": "OpenCVë¥¼ í™œìš©í•˜ì—¬ íƒë³¸ ì´ë¯¸ì§€ì˜ ëŒ€ë¹„ í–¥ìƒê³¼ ì´ì§„í™” ë“± ì¼ë ¨ì˜ ì „ì²˜ë¦¬ ë‹¨ê³„ ìˆ˜í–‰",
        "summary": "ë…¸ì´ì¦ˆ ì–µì œ, ì¡°ëª… ë³´ì •, ê¸€ì ê·¹ì„± íŒë³„, ëŒ€ë¹„ ê°•í™”, ì–¸ìƒ¤í”„, ì´ì§„í™”, ëª¨í´ë¡œì§€, ë°°ê²½ ë°˜ì „ ë“±ì„ í¬í•¨í•œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„",
        "files": {
            "preprocess_takbon_safe.py": r'''import cv2
import numpy as np
import json
from pathlib import Path

def save_img(p, img):
    Path(p).parent.mkdir(parents=True, exist_ok=True)
    cv2.imwrite(str(p), img)

def gentle_unsharp(gray, radius=3, amount=0.45):
    blur = cv2.GaussianBlur(gray, ((radius|1), (radius|1)), 0)
    sharp = cv2.addWeighted(gray, 1.0 + amount, blur, -amount, 0)
    return sharp

def linear_stretch(gray, lo=1.0, hi=99.0):
    p1, p2 = np.percentile(gray, [lo, hi])
    if p2 <= p1 + 1e-6:
        return gray
    out = np.clip((gray - p1) * (255.0 / (p2 - p1)), 0, 255).astype(np.uint8)
    return out

def estimate_text_polarity(gray):
    h, w = gray.shape
    edges = cv2.Canny(gray, 50, 150)
    k = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
    band = cv2.dilate(edges, k, iterations=1)
    edge_vals = gray[band > 0]
    bg_mask = cv2.erode((band == 0).astype(np.uint8), k, iterations=2)
    bg_vals = gray[bg_mask > 0]
    if len(edge_vals) < 100 or len(bg_vals) < 100:
        return (gray.mean() < 120)
    return (edge_vals.mean() > bg_vals.mean())

def preprocess_takbon_safe(image_path, out_dir="./out_safe2"):
    name = Path(image_path).stem
    outdir = Path(out_dir)/name
    outdir.mkdir(parents=True, exist_ok=True)
    meta = {"file": str(image_path)}
    src = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
    if src is None:
        raise FileNotFoundError(image_path)
    save_img(outdir/"00_src.png", src)
    den = cv2.medianBlur(src, 3)
    save_img(outdir/"01_denoise.png", den)
    k = 71 if min(src.shape) > 1200 else 41
    if k % 2 == 0: k += 1
    bg = cv2.medianBlur(den, k)
    norm = cv2.normalize((den.astype(np.float32) / (bg.astype(np.float32) + 1e-6)), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
    save_img(outdir/"02_illum_norm.png", norm)
    need_invert = estimate_text_polarity(norm)
    gray = cv2.bitwise_not(norm) if need_invert else norm
    meta["invert_applied"] = bool(need_invert)
    save_img(outdir/"03_gray_after_polarity.png", gray)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    c1 = clahe.apply(gray)
    c2 = linear_stretch(c1, 2.0, 98.0)
    save_img(outdir/"04_contrast.png", c2)
    sh = gentle_unsharp(c2, radius=3, amount=0.35)
    save_img(outdir/"05_sharp.png", sh)
    H, W = sh.shape
    win = int(max(25, (min(H, W)//48) | 1))
    bin_adp = cv2.adaptiveThreshold(sh, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, win, 8)
    _, bin_otsu = cv2.threshold(sh, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    def balance_score(b):
        p = float(np.mean(b == 255))
        return -abs(p - 0.5)
    bin_final = bin_adp if balance_score(bin_adp) >= balance_score(bin_otsu) else bin_otsu
    save_img(outdir/"06_bin_raw.png", bin_final)
    open_k = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))
    bin_clean = cv2.morphologyEx(bin_final, cv2.MORPH_OPEN, open_k, iterations=2)
    save_img(outdir/"07_bin_clean.png", bin_clean)
    white_ratio = float(np.mean(bin_clean == 255))
    if white_ratio < 0.5:
        bin_clean = cv2.bitwise_not(bin_clean)
    save_img(outdir/f"{name}_ocrprep.png", bin_clean)
    save_img(outdir/f"{name}_master.png", sh)
    meta.update({
        "illum_kernel": int(k),
        "adaptive_win": int(win),
        "white_ratio": white_ratio
    })
    with open(outdir/"params.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"âœ… ì™„ë£Œ: {image_path} â†’ {outdir}")
    return outdir

if __name__ == "__main__":
    image_name = "test3.jpg"
    preprocess_takbon_safe(image_name)
'''
        }
    },
    {
        "path": "vision/preprocessing/briefnet",
        "title": "briefnetì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬",
        "purpose": "BiRefNet ë“± ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë¡œ ì „ê²½ì„ ë¶„ë¦¬í•˜ì—¬ ë°°ê²½ì„ ì œê±°",
        "summary": "ë”¥ëŸ¬ë‹ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ì´ìš©í•´ í•œì ì˜ì—­ë§Œ ë¶„ë¦¬í•˜ê³  ë§ˆìŠ¤í¬ë¥¼ ì´ìš©í•œ ë°°ê²½ì œê±°ë¥¼ êµ¬í˜„",
        "files": {
            "birefnet_segmentation.py": r'''from transformers import AutoModelForImageSegmentation, AutoImageProcessor
from PIL import Image
import requests
import matplotlib.pyplot as plt
import numpy as np
import torch
from pathlib import Path

def run_birefnet_segmentation(img_path: str, model_id: str = "Zhengpeng7/BiRefNet"):
    processor = AutoImageProcessor.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForImageSegmentation.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()
    img_path_obj = Path(img_path)
    if img_path_obj.is_file():
        img = Image.open(img_path_obj).convert("RGB")
    else:
        # fallback: download a demo image
        demo_url = "https://huggingface.co/datasets/hf-internal-testing/dummy-image-classification/resolve/main/imagenet_classification/000000039769.png"
        img = Image.open(requests.get(demo_url, stream=True).raw).convert("RGB")
    inputs = processor(images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    if hasattr(outputs, "logits"):
        logits = outputs.logits
    elif isinstance(outputs, (list, tuple)):
        logits = outputs[0]
    else:
        logits = outputs
    if logits.shape[1] == 1:
        pred_mask = torch.sigmoid(logits[0, 0])
    else:
        probs = torch.softmax(logits[0], dim=0)
        fg_class = 1
        pred_mask = probs[fg_class]
    pred_mask_np = pred_mask.detach().cpu().numpy()
    mask_bin = (pred_mask_np > 0.5).astype(np.uint8)
    img_np = np.array(img)
    mask_3ch = np.repeat(mask_bin[..., None], 3, axis=2)
    white_bg = np.ones_like(img_np, dtype=np.uint8) * 255
    seg_result = np.where(mask_3ch == 1, img_np, white_bg)
    return img_np, pred_mask_np, seg_result
'''
        }
    },
    {
        "path": "vision/preprocessing/DBNet",
        "title": "DBNetì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬",
        "purpose": "PaddleOCRì˜ DBNet ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ì ì˜ì—­ì„ ê²€ì¶œí•˜ê³  ë°°ê²½ì„ ì œê±°",
        "summary": "CLAHE ëŒ€ë¹„ ê°•í™” í›„ DBNetìœ¼ë¡œ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ê²€ì¶œí•˜ê³  ë§ˆìŠ¤í¬ë¥¼ ë§Œë“¤ì–´ í° ë°°ê²½ìœ¼ë¡œ í•©ì„±",
        "files": {
            "dbnet_preprocess.py": r'''from paddleocr import PaddleOCR
import cv2
import numpy as np
from pathlib import Path

ocr = PaddleOCR(use_angle_cls=False, use_gpu=False, det=True, rec=False, lang='ch')

def text_cutout_whitebg(src_path, dst_path=None, contrast_boost=True):
    src_path = Path(src_path)
    img = cv2.imread(str(src_path))
    if img is None:
        raise FileNotFoundError(src_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    if contrast_boost:
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        gray = clahe.apply(gray)
    tmp_path = src_path.with_name(src_path.stem + "_tmp_for_ocr.png")
    cv2.imwrite(str(tmp_path), gray)
    res = ocr.ocr(str(tmp_path), det=True, rec=False)
    if not res or res[0] is None or len(res[0]) == 0:
        print("âš ï¸ ê¸€ì ë°•ìŠ¤ê°€ ê²€ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëŒ€ë¹„ë‚˜ ë°ê¸° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None
    mask = np.zeros(img.shape[:2], dtype=np.uint8)
    for line in res[0]:
        if line is None:
            continue
        points = np.array(line[0]).astype(np.int32)
        cv2.fillPoly(mask, [points], 255)
    white_bg = np.ones_like(img, dtype=np.uint8) * 255
    mask_3 = cv2.merge([mask]*3)
    result = np.where(mask_3 == 255, img, white_bg)
    if dst_path is None:
        dst_path = src_path.with_name(src_path.stem + "_text_cutout.png")
    cv2.imwrite(str(dst_path), result)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {dst_path}")
    tmp_path.unlink(missing_ok=True)
    return str(dst_path)
'''
        }
    },
    {
        "path": "vision/preprocessing/U2Net",
        "title": "UÂ²-Netì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬",
        "purpose": "UÂ²-Net ëª¨ë¸ë¡œ ë°°ê²½ ì œê±°ì™€ pepper noise ì œê±°ë¥¼ ìˆ˜í–‰",
        "summary": "rembgë¥¼ ì´ìš©í•´ RGBA ì»·ì•„ì›ƒì„ ì–»ê³  OpenCVë¡œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œ í›„ í° ë°°ê²½ìœ¼ë¡œ í•©ì„±",
        "files": {
            "u2net_preprocess.py": r'''from rembg import remove, new_session
from PIL import Image
import numpy as np
import cv2
from pathlib import Path

# ì„¸ì…˜ ì´ˆê¸°í™” (ì •í™•ë„ ëª¨ë“œ)
session = new_session("u2net")

def takbon_cutout_and_clean(src_path, dst_path=None, noise_kernel=3, min_area=20):
    src = Path(src_path)
    if dst_path is None:
        dst_path = src.with_name(src.stem + "_white_bg.png")
    im = Image.open(src).convert("RGBA")
    removed = remove(im, session=session)
    rgba = np.array(removed)
    alpha = rgba[:, :, 3]
    rgb = rgba[:, :, :3]
    _, mask = cv2.threshold(alpha, 0, 255, cv2.THRESH_BINARY)
    kernel = np.ones((noise_kernel, noise_kernel), np.uint8)
    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_clean)
    mask_final = np.zeros_like(mask_clean)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area > min_area:
            mask_final[labels == i] = 255
    white_bg = np.ones_like(rgb, dtype=np.uint8) * 255
    mask_3ch = cv2.merge([mask_final]*3)
    result = np.where(mask_3ch == 255, rgb, white_bg)
    Image.fromarray(result).save(dst_path)
    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {dst_path}")
    return str(dst_path)
'''
        }
    },
    {
        "path": "ocr/craft/CRAFT",
        "title": "CRAFTë¥¼ ì´ìš©í•œ í•œì ì¸ì‹",
        "purpose": "CRAFT íƒì§€ê¸°ë¡œ í•œì ì˜ì—­ì„ ê²€ì¶œí•˜ê³  ë­‰í……ì´ë¥¼ watershedë¡œ ë¶„í• ",
        "summary": "CRAFT ë²„ì „ í˜¸í™˜ì„ ê°ì•ˆí•´ detect_textë¥¼ í˜¸ì¶œí•˜ê³ , í° ì˜ì—­ì€ ê±°ë¦¬ë³€í™˜+watershedë¡œ ë‚˜ëˆ  ì‘ì€ ê¸€ìë¥¼ ì¶”ì¶œ",
        "files": {
            "craft_ocr.py": r'''# -*- coding: utf-8 -*-
"""
CRAFT í•œì(í…ìŠ¤íŠ¸) íƒì§€ + ë­‰í……ì´ ë¶„í• (watershed) ìŠ¤í¬ë¦½íŠ¸
- craft-text-detector ë²„ì „ ì°¨ì´ ìë™ ëŒ€ì‘
- í° ë©ì–´ë¦¬(ë¶™ì€ ê¸€ìë“¤)ë¥¼ ê±°ë¦¬ë³€í™˜+watershedë¡œ ê¸€ì ë‹¨ìœ„ë¡œ ë¶„í• 
- ê²°ê³¼: ì˜¤ë²„ë ˆì´ PNG, ë°•ìŠ¤ JSON, (ì˜µì…˜) í¬ë¡­ ì €ì¥
"""
import json
import inspect
from pathlib import Path
import cv2
import numpy as np
from craft_text_detector import Craft

IMG_PATH = r"C:\Users\myjew\takbon\test5.png"
OUT_DIR  = r"C:\hanja_craft_out"
SAVE_CROPS = True
PARAMS = dict(
    text_threshold=0.72,
    low_text=0.40,
    link_threshold=0.20,
    long_size=1920,
    cuda=False,
    refiner=False,
)
MIN_BOX_W, MIN_BOX_H = 8, 8
SPLIT_ENABLE = True
AREA_FACTOR = 3.0
SPLIT_MIN_CHAR_PIXELS = 8
SPLIT_MAX_CHARS = 32

def sort_poly_clockwise(poly):
    poly = np.array(poly, dtype=np.float32)
    c = np.mean(poly, axis=0)
    ang = np.arctan2(poly[:, 1] - c[1], poly[:, 0] - c[0])
    idx = np.argsort(ang)
    return poly[idx].tolist()

def poly_to_bbox(poly):
    xs = [p[0] for p in poly]
    ys = [p[1] for p in poly]
    x1, y1 = int(min(xs)), int(min(ys))
    x2, y2 = int(max(xs)), int(max(ys))
    return [x1, y1, int(x2 - x1), int(y2 - y1)]

def as_list(x):
    if x is None:
        return []
    if isinstance(x, np.ndarray):
        return x.tolist()
    return x

def run_craft_detect(img_path: str, params: dict):
    craft = Craft(cuda=params.get("cuda", False), refiner=params.get("refiner", False))
    sig = inspect.signature(craft.detect_text)
    accepts = set(sig.parameters.keys())
    if {"text_threshold", "low_text", "link_threshold", "long_size"}.issubset(accepts):
        prediction = craft.detect_text(
            img_path,
            text_threshold=params["text_threshold"],
            low_text=params["low_text"],
            link_threshold=params["link_threshold"],
            long_size=params["long_size"],
        )
        return craft, prediction
    craft = Craft(
        cuda=params.get("cuda", False),
        refiner=params.get("refiner", False),
        text_threshold=params.get("text_threshold", 0.7),
        low_text=params.get("low_text", 0.4),
        link_threshold=params.get("link_threshold", 0.4),
        long_size=params.get("long_size", 1280),
    )
    prediction = craft.detect_text(img_path)
    return craft, prediction

def split_blob_into_chars(crop_bgr, min_char=8, max_chars=16):
    gray = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2GRAY)
    _, binv = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    k = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
    binv = cv2.morphologyEx(binv, cv2.MORPH_OPEN, k, iterations=1)
    dist = cv2.distanceTransform(binv, cv2.DIST_L2, 3)
    dist_norm = cv2.normalize(dist, None, 0, 1.0, cv2.NORM_MINMAX)
    _, peaks = cv2.threshold((dist_norm * 255).astype(np.uint8), 120, 255, cv2.THRESH_BINARY)
    peaks = cv2.morphologyEx(peaks, cv2.MORPH_OPEN, k, iterations=1)
    n_labels, markers = cv2.connectedComponents(peaks)
    markers = markers + 1
    markers[binv == 0] = 0
    ws_in = cv2.cvtColor(binv, cv2.COLOR_GRAY2BGR)
    cv2.watershed(ws_in, markers)
    boxes = []
    for lab in range(2, n_labels + 1):
        mask = (markers == lab).astype(np.uint8) * 255
        if cv2.countNonZero(mask) < min_char:
            continue
        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not cnts:
            continue
        x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))
        if w >= min_char and h >= min_char:
            boxes.append((x, y, w, h))
    if len(boxes) > max_chars:
        boxes = sorted(boxes, key=lambda b: b[2] * b[3], reverse=True)[:max_chars]
    boxes = sorted(boxes, key=lambda b: (b[1], b[0]))
    return boxes

def main():
    out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)
    overlay_dir = out_dir / "overlay"; overlay_dir.mkdir(exist_ok=True)
    crops_dir = out_dir / "crops"; crops_dir.mkdir(exist_ok=True)
    img_bgr = cv2.imread(IMG_PATH, cv2.IMREAD_COLOR)
    assert img_bgr is not None, f"ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {IMG_PATH}"
    H, W = img_bgr.shape[:2]
    craft = None
    try:
        craft, prediction = run_craft_detect(IMG_PATH, PARAMS)
        polys = as_list(prediction.get("polys"))
        boxes = as_list(prediction.get("boxes"))
        regions = polys if len(polys) > 0 else boxes
        raw = []
        areas = []
        for region in regions:
            poly = sort_poly_clockwise(region)
            x, y, bw, bh = poly_to_bbox(poly)
            if bw < MIN_BOX_W or bh < MIN_BOX_H:
                continue
            raw.append((poly, x, y, bw, bh))
            areas.append(bw * bh)
        median_area = float(np.median(areas)) if areas else 0.0
        results = []
        vis = img_bgr.copy()
        for poly, x, y, bw, bh in raw:
            area = bw * bh
            split_done = False
            if SPLIT_ENABLE and median_area > 0 and area >= AREA_FACTOR * median_area:
                x1, y1 = max(0, x), max(0, y)
                x2, y2 = min(W, x + bw), min(H, y + bh)
                crop = img_bgr[y1:y2, x1:x2]
                if crop.size > 0:
                    char_boxes = split_blob_into_chars(
                        crop,
                        min_char=SPLIT_MIN_CHAR_PIXELS,
                        max_chars=SPLIT_MAX_CHARS
                    )
                    if len(char_boxes) >= 2:
                        split_done = True
                        for (cx, cy, cw, ch) in char_boxes:
                            gx, gy, gw, gh = x + cx, y + cy, cw, ch
                            if gw < MIN_BOX_W or gh < MIN_BOX_H:
                                continue
                            results.append({
                                "index": len(results) + 1,
                                "poly": [[gx, gy], [gx+gw, gy], [gx+gw, gy+gh], [gx, gy+gh]],
                                "bbox": [int(gx), int(gy), int(gw), int(gh)],
                                "split": True
                            })
                            cv2.rectangle(vis, (gx, gy), (gx+gw, gy+gh), (0, 200, 255), 2)
                            cv2.putText(vis, f"{len(results)}", (gx, max(0, gy-5)),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)
                            if SAVE_CROPS:
                                gx1, gy1 = max(0, gx), max(0, gy)
                                gx2, gy2 = min(W, gx+gw), min(H, gy+gh)
                                gcrop = img_bgr[gy1:gy2, gx1:gx2]
                                if gcrop.size > 0:
                                    cv2.imwrite(str(crops_dir / f"char_{len(results):04d}.png"), gcrop)
            if not split_done:
                results.append({
                    "index": len(results) + 1,
                    "poly": [[int(px), int(py)] for px, py in poly],
                    "bbox": [int(x), int(y), int(bw), int(bh)],
                    "split": False
                })
                pts = np.array(poly, dtype=np.int32).reshape(-1, 1, 2)
                cv2.polylines(vis, [pts], isClosed=True, color=(0, 255, 0), thickness=2)
                cv2.rectangle(vis, (x, y), (x + bw, y + bh), (255, 0, 0), 1)
                cv2.putText(vis, f"{len(results)}", (x, max(0, y - 5)),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)
                if SAVE_CROPS:
                    x1, y1 = max(0, x), max(0, y)
                    x2, y2 = min(W, x + bw), min(H, y + bh)
                    crop = img_bgr[y1:y2, x1:x2]
                    if crop.size > 0:
                        cv2.imwrite(str(crops_dir / f"blob_{len(results):04d}.png"), crop)
        base = Path(IMG_PATH).stem
        cv2.imwrite(str((Path(OUT_DIR) / "overlay" / f"{base}_overlay.png")), vis)
        with open(Path(OUT_DIR) / f"{base}_boxes.json", "w", encoding="utf-8") as f:
            json.dump({
                "image": str(Path(IMG_PATH)),
                "size": {"w": W, "h": H},
                "count": len(results),
                "median_area": median_area,
                "area_factor": AREA_FACTOR,
                "split_enable": SPLIT_ENABLE,
                "results": results
            }, f, ensure_ascii=False, indent=2)
        print(f"[ì™„ë£Œ] ì´ {len(results)}ê°œ ë°•ìŠ¤ (ë¶„í•  í¬í•¨)")
        print(f" - ì˜¤ë²„ë ˆì´: {Path(OUT_DIR) / 'overlay' / (base + '_overlay.png')}")
        print(f" - ë°•ìŠ¤ JSON: {Path(OUT_DIR) / (base + '_boxes.json')}")
        if SAVE_CROPS:
            print(f" - í¬ë¡­ í´ë”: {Path(OUT_DIR) / 'crops'}")
    finally:
        if craft is not None:
            try: craft.unload_craftnet_model()
            except Exception: pass
            try: craft.unload_refinenet_model()
            except Exception: pass

if __name__ == "__main__":
    main()
'''
        }
    },
    {
        "path": "nlp/sikuroberta/Colab_ë°±ì—…",
        "title": "[ì°¸ê³ ìš©] Colabìš© ì½”ë“œ ë°±ì—…",
        "purpose": "Colabì—ì„œ ì‘ì„±ëœ ì½”ë“œì˜ ë°±ì—…ë³¸",
        "summary": "íƒë³¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµì— ì‚¬ìš©ëœ ì—¬ëŸ¬ Colab ë…¸íŠ¸ë¶ ì½”ë“œì˜ ëª¨ìŒì…ë‹ˆë‹¤.",
        "files": {
            "README.md": "# ì°¸ê³ ìš© Colab ì½”ë“œ\n\nì´ í´ë”ì—ëŠ” Colabì—ì„œ ì‹¤í–‰í–ˆë˜ ì „ì²˜ë¦¬ ë° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ ë°±ì—…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ìŠ¤í¬ë¦½íŠ¸ëŠ” Notion í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì‹  êµ¬ì¡°ë¡œ ë¦¬íŒ©í† ë§ ë˜ê¸° ì „ì— ì‚¬ìš©ëœ ì‹¤í—˜ ì½”ë“œì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "nlp/translation/ExaOne_í‰ê°€",
        "title": "ExaOne ì„±ëŠ¥ í‰ê°€ ì½”ë“œ",
        "purpose": "Gemini ë²ˆì—­ ëª¨ë¸ ì¤‘ ExaOneì„ ì‚¬ìš©í•œ ë²ˆì—­ ì„±ëŠ¥ í‰ê°€",
        "summary": "ìœ„ì˜ nlp/gemini/ExaOne í´ë”ì™€ ì¤‘ë³µë˜ì–´ ìˆì§€ë§Œ ì˜ˆì œ ìœ ì§€ìš©ìœ¼ë¡œ ìƒì„±.",
        "files": {
            "README.md": "# ExaOne ë²ˆì—­ í‰ê°€ ì½”ë“œ\n\nGemini ë²ˆì—­ ì‹¤í—˜ì„ ìœ„í•´ ExaOne ëª¨ë¸ì„ ì´ìš©í•´ ë²ˆì—­ ë° BLEU í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "image/briefnet/briefnet_ì„_ì´ìš©í•œ_ì´ë¯¸ì§€_ì „ì²˜ë¦¬",
        "title": "briefnetì„ ì´ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬",
        "purpose": "BiRefNet ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ íƒë³¸ ì´ë¯¸ì§€ì˜ ì „ê²½ì„ ë¶„ë¦¬",
        "summary": "BiRefNet ëª¨ë¸ë¡œ segmentation maskë¥¼ ìƒì„±í•˜ê³ , thresholdingìœ¼ë¡œ ê¸€ì ì˜ì—­ì„ ì¶”ì¶œí•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œë¥¼ í°ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
        "files": {
            "README.md": "# briefnet ì „ì²˜ë¦¬\n\nì´ í´ë”ì—ëŠ” BiRefNet ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ ì´ìš©í•´ íƒë³¸ ì´ë¯¸ì§€ì˜ ì „ê²½(í•œì)ê³¼ ë°°ê²½ì„ ë¶„ë¦¬í•˜ëŠ” ì˜ˆì œê°€ í¬í•¨ë©ë‹ˆë‹¤. ìƒì„¸ ì½”ë“œëŠ” birefnet_segmentation.pyë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
        }
    },
    {
        "path": "ocr/paddle/Paddle_OCR",
        "title": "Paddle OCR",
        "purpose": "PaddleOCRë¥¼ ì´ìš©í•œ í•œì OCR ì‹¤í—˜",
        "summary": "PaddleOCRì˜ det+rec ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°•ìŠ¤ë¥¼ ê²€ì¶œí•˜ê³  í•œì ì¸ì‹ì„ ìˆ˜í–‰í•˜ëŠ” ì—¬ëŸ¬ ì‹œí–‰ì°©ì˜¤ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.",
        "files": {
            "methodB.py": "# Paddle OCR Method B\n\nì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Notion í˜ì´ì§€ì—ì„œ ìƒì„¸íˆ ì„¤ëª…ëœ PaddleOCR(det+rec) íŒŒì´í”„ë¼ì¸ì˜ ìš”ì•½ë³¸ì…ë‹ˆë‹¤. ì „ì²´ ì½”ë“œëŠ” ë…¸ì…˜ì„ ì°¸ê³ í•˜ì„¸ìš”."
        }
    },
    {
        "path": "ocr/paddle_ensemble/Paddle_OCR_ê³¼_ê³ ë¬¸ì„œOCR_ì•™ìƒë¸”",
        "title": "Paddle OCRê³¼ ê³ ë¬¸ì„œOCR ì•™ìƒë¸”",
        "purpose": "PaddleOCR ê²°ê³¼ì™€ ê³ ë¬¸ì„œ OCR(HRCN) ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•˜ì—¬ í•œì ì¸ì‹ë¥  í–¥ìƒì„ ê¾€í•¨",
        "summary": "ì„¸ë¡œì“°ê¸° ì •ë ¬ê³¼ ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ë‘ OCR ì—”ì§„ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.",
        "files": {
            "fusion.py": "# Fusion Placeholder\n\nì´ íŒŒì¼ì€ PaddleOCRì™€ HRCN OCRì˜ ì•™ìƒë¸” ì˜ˆì œì— ëŒ€í•œ ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤. ì‹¤ì œ ì½”ë“œ êµ¬í˜„ì€ Notion í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
        }
    },
    {
        "path": "ocr/aihub/AI_Hub_ê³ ë¬¸ì„œ_OCR_ë‹¨ë…_ì‹¤í–‰_ì½”ë“œ",
        "title": "AI Hub ê³ ë¬¸ì„œ OCR ë‹¨ë… ì‹¤í–‰ ì½”ë“œ",
        "purpose": "AI Hub ê³ ë¬¸ì„œ OCR ëª¨ë¸ì„ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸",
        "summary": "ResNet ê¸°ë°˜ íƒì§€ ëª¨ë¸ê³¼ ì¸ì‹ê¸°ë¥¼ ë¡œë“œí•˜ì—¬ í•œì OCR ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "files": {
            "aihub_ocr.py": "# AI Hub OCR Placeholder\n\nAI Hub ê³ ë¬¸ì„œ OCR ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” ì‹¤ì œ ì½”ë“œëŠ” Notion í˜ì´ì§€ì— ìì„¸íˆ ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ëŠ” ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "ocr/easyocr/Easy_OCR_í•œì_í•œ_ê¸€ì_ì¸ì‹",
        "title": "Easy OCR í•œì í•œ ê¸€ì ì¸ì‹",
        "purpose": "EasyOCRë¥¼ í™œìš©í•˜ì—¬ í•œì í•œ ê¸€ì ë‹¨ìœ„ë¡œ ì¸ì‹í•˜ëŠ” ì‹¤í—˜",
        "summary": "EasyOCR ì—”ì§„ê³¼ ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ í•œì 1ê¸€ì ë°ì´í„°ì…‹ ìƒì„± ê°€ëŠ¥ì„±ì„ íƒìƒ‰.",
        "files": {
            "easyocr_experiment.py": "# EasyOCR Placeholder\n\nEasyOCRë¥¼ ì´ìš©í•œ í•œì í•œ ê¸€ì ì¸ì‹ ì‹¤í—˜ ì½”ë“œëŠ” Notion í˜ì´ì§€ì— ìˆìŠµë‹ˆë‹¤."
        }
    },
    {
        "path": "ocr/hrnet/Faster_RCNN_HRNet",
        "title": "Faster-RCNN + HRNet",
        "purpose": "HRNet ë°±ë³¸ì˜ Faster R-CNNìœ¼ë¡œ í•œì ê°ì²´ ê²€ì¶œ ì‹¤í—˜",
        "summary": "MMDetection í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œì ê°ì²´ ê²€ì¶œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.",
        "files": {
            "faster_rcnn_hrnet.py": "# Faster R-CNN + HRNet Placeholder\n\ní•´ë‹¹ ì‹¤í—˜ì„ ìœ„í•œ MMDetection ì„¤ì • ë° í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ëŠ” Notion í˜ì´ì§€ì™€ MMDetection ì„¤ì • íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”."
        }
    },
    {
        "path": "ocr/hrnet/Faster_RCNN_HRNet_Crop_augmentation",
        "title": "Faster R-CNN+HRNet Crop augmentation",
        "purpose": "í¬ë¡­ ì¦ê°•ì„ ì¶”ê°€í•œ Faster R-CNN+HRNet í›ˆë ¨",
        "summary": "ëœë¤ í¬ë¡­ì„ í™œìš©í•´ ì‘ì€ ê¸€ì ê²€ì¶œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ê³¼ì í•©ì„ ì™„í™”í•˜ë ¤ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.",
        "files": {
            "crop_aug_script.py": "# Crop Augmentation Placeholder\n\nFaster R-CNN+HRNet ëª¨ë¸ì— í¬ë¡­ ì¦ê°•ì„ ì ìš©í•œ ì½”ë“œì˜ ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "ocr/fcos/FCOS",
        "title": "FCOS",
        "purpose": "FCOS one-stage ê²€ì¶œê¸°ë¥¼ í•œì ê²€ì¶œì— ì ìš©í•˜ëŠ” ì‹¤í—˜",
        "summary": "Detectron2 ê¸°ë°˜ FCOS ëª¨ë¸ë¡œ í•œì ìœ„ì¹˜ë¥¼ íƒì§€í•˜ëŠ” ì´ˆê¸° ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤.",
        "files": {
            "fcos_experiment.py": "# FCOS Placeholder\n\nFCOS ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹¤í—˜ ì½”ë“œì˜ ìì„¸í•œ ë‚´ìš©ì€ Notion í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
        }
    },
    {
        "path": "ocr/yolo/YOLO",
        "title": "YOLO",
        "purpose": "YOLO ëª¨ë¸ì„ í™œìš©í•œ í•œì ì˜ì—­ íƒì§€ ì‹¤í—˜",
        "summary": "Ultralytics YOLOë¥¼ ì´ìš©í•´ ë¹ ë¥¸ ì‹¤í—˜ì„ ì§„í–‰í•˜ê³  í•œì ê²€ì¶œ ê°€ëŠ¥ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.",
        "files": {
            "yolo_experiment.py": "# YOLO Placeholder\n\nYOLO ê¸°ë°˜ í•œì ê²€ì¶œ ì‹¤í—˜ì„ ìœ„í•œ ì½”ë“œ ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "ocr/kakren/Kakren_CHAT_OCR",
        "title": "Kakren(CHAT OCR)",
        "purpose": "Kraken OCRê³¼ CHAT ëª¨ë¸ì„ í™œìš©í•´ í•œì OCR ì„±ëŠ¥ì„ í‰ê°€",
        "summary": "Kraken ë° CHAT OCR ëª¨ë¸ì„ ì„¤ì¹˜í•˜ê³  CLIì™€ Python APIë¡œ í•œì ê³ ë¬¸ì„œ OCRì„ ìˆ˜í–‰í•œ ì‹¤í—˜ì…ë‹ˆë‹¤.",
        "files": {
            "kakren_ocr.py": "# Kakren OCR Placeholder\n\nKraken/CHAT OCR ì‹¤í—˜ ì½”ë“œëŠ” Notion í˜ì´ì§€ë¥¼ ì°¸ê³ í•˜ì‹­ì‹œì˜¤."
        }
    },
    {
        "path": "ocr/deepseek/DeepSeek_OCR",
        "title": "DeepSeek OCR",
        "purpose": "DeepSeek OCR ì—”ì§„ì„ í™œìš©í•˜ì—¬ í•œì OCR ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸",
        "summary": "DeepSeek OCRì˜ ì¶œë ¥ í˜•íƒœë¥¼ ë¶„ì„í•˜ê³  í›„ì²˜ë¦¬ ê¸°ë²•ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ ì„ ì‹œë„í•˜ëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.",
        "files": {
            "deepseek_ocr.py": "# DeepSeek OCR Placeholder\n\nDeepSeek OCR ì—”ì§„ í…ŒìŠ¤íŠ¸ ì½”ë“œ ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤."
        }
    },
    {
        "path": "ocr/google/Google_OCR",
        "title": "Google OCR",
        "purpose": "Google Cloud Vision OCRì„ í™œìš©í•œ í•œì ì¸ì‹ ë° í›„ì²˜ë¦¬ ì‹¤í—˜",
        "summary": "Google OCR ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ë£° ê¸°ë°˜ í›„ì²˜ë¦¬ì™€ ê³ ë¬¸ì„œ OCR ì•™ìƒë¸”ì„ í…ŒìŠ¤íŠ¸í•œ ì½”ë“œ ì‹¤í—˜ì…ë‹ˆë‹¤.",
        "files": {
            "google_ocr.py": "# Google OCR Placeholder\n\nGoogle Cloud Vision OCRì— ëŒ€í•œ í›„ì²˜ë¦¬ ë° ì•™ìƒë¸” ì½”ë“œì˜ ìë¦¬í‘œì‹œìì…ë‹ˆë‹¤."
        }
    }
]

def remove_emojis(text: str) -> str:
    """
    ë¬¸ìì—´ì—ì„œ ì´ëª¨ì§€(emojis)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

    Emoji ë²”ìœ„ëŠ” ì—¬ëŸ¬ Unicode ë¸”ë¡ì— ê±¸ì³ ìˆìœ¼ë¯€ë¡œ ì •ê·œì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ëŒ€ë¶€ë¶„ì˜ ì´ëª¨ì§€ëŠ” U+1F300~U+1F6FF, U+1F900~U+1F9FF ë“±ì— í¬í•¨ë©ë‹ˆë‹¤.

    Args:
        text (str): ì›ë³¸ ë¬¸ìì—´

    Returns:
        str: ì´ëª¨ì§€ê°€ ì œê±°ëœ ë¬¸ìì—´
    """
    # ì´ëª¨ì§€ íŒ¨í„´ ì •ì˜
    emoji_pattern = re.compile(
        "["
        "\U0001F300-\U0001F5FF"  # ê¸°í˜¸ ë° ì•„ì´ì½˜
        "\U0001F600-\U0001F64F"  # ì´ëª¨í‹°ì½˜ í‘œì •
        "\U0001F680-\U0001F6FF"  # êµí†µ ë° ì§€ë„ ê¸°í˜¸
        "\U0001F700-\U0001F77F"  # ê¸°í˜¸ ë° ì•„ì´ì½˜ í™•ì¥
        "\U0001F780-\U0001F7FF"  # ì¶”ê°€ ê¸°í˜¸
        "\U0001F800-\U0001F8FF"  # ì¶”ê°€ í™”ì‚´í‘œ
        "\U0001F900-\U0001F9FF"  # ë³´ì¶© ê¸°í˜¸ ë° í”½í† ê·¸ë¨
        "\U0001FA70-\U0001FAFF"  # ìŒì‹/ê¸°íƒ€ ê¸°í˜¸
        "\U00002702-\U000027B0"  # íŠ¹ìˆ˜ ê¸°í˜¸
        "\U000024C2-\U0001F251"  # ê¸°í˜¸
        "]+",
        flags=re.UNICODE,
    )
    return emoji_pattern.sub('', text)


def main():
    """
    ë©”ì¸ í•¨ìˆ˜ëŠ” ì‹¤í—˜ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ê° ë””ë ‰í„°ë¦¬ë¥¼ ìƒì„±í•˜ê³  README ë° ì½”ë“œ íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤.

    - ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.
    - READMEì™€ ì½”ë“œ íŒŒì¼ì—ì„œ ì´ëª¨ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    - ê° ì½”ë“œ íŒŒì¼ ìƒë‹¨ì— í•œêµ­ì–´ ì£¼ì„ì„ ì¶”ê°€í•˜ì—¬ ì œëª©, ëª©ì , ìš”ì•½ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
    """
    created_files = []
    for exp in experiments:
        # ì‹¤í—˜ìš© í´ë” ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        dir_path = os.path.join(BASE_PATH, *exp['path'].split('/'))
        os.makedirs(dir_path, exist_ok=True)
        # README ë‚´ìš© ì‘ì„± (ì´ëª¨ì§€ ì œê±°)
        readme_content = (
            f"# {exp['title']}\n\n"
            f"## ëª©ì \n"
            f"- {exp['purpose']}\n\n"
            f"## ì‹œí–‰ì°©ì˜¤ ìš”ì•½\n"
            f"- {exp['summary']}\n"
        )
        readme_content = remove_emojis(readme_content)
        readme_path = os.path.join(dir_path, "README.md")
        # ê¸°ì¡´ READMEê°€ ìˆìœ¼ë©´ ì‚­ì œ
        if os.path.exists(readme_path):
            os.remove(readme_path)
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(readme_content)
        created_files.append(readme_path)
        # ì½”ë“œ íŒŒì¼ ìƒì„±
        for filename, content in exp['files'].items():
            file_path = os.path.join(dir_path, filename)
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
            if os.path.exists(file_path):
                os.remove(file_path)
            # ì´ëª¨ì§€ ì œê±° ë° ì£¼ì„ í—¤ë” ì¶”ê°€
            content_clean = remove_emojis(content)
            header_lines = [
                f"# {exp['title']}",
                f"# ëª©ì : {exp['purpose']}",
                f"# ìš”ì•½: {exp['summary']}",
                f"# ì‘ì„±ì¼: 2025-12-10",
                "",
            ]
            header = "\n".join(header_lines)
            content_with_header = header + content_clean
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content_with_header)
            created_files.append(file_path)
    # ìƒì„±ëœ ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì¶œë ¥
    for path in created_files:
        print(path)

if __name__ == "__main__":
    main()
